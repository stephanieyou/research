---
layout: post
title: Report 2
categories: [report]
comments: false
---

#### Progress since Report 1
My last report was from 12/16/2018, and I stated:

> The next steps are to obtain access to CMU clusters through Professor Wehbe
> and decide which word embedding algorithms to use next semester. Since most of
> the fourteen English word embedding modules on TensorFlow Hub are not trained
> with the same test corpus, I also need to investigate how to obtain pre-
> trained models for each of the algorithms I decide to use.

I am currently in the process of obtaining access to the CMU clusters that
Professor Wehbe uses. I submitted a request for an SCS account and am waiting
for the SCS Help Desk to approve it.

#### Papers Read
In the last week, I have read the following two papers:
* Zhu et al. "Exploring Semantic Properties of Sentence Embeddings"
* Conneau et al. "What you can cram into a single $&!#* vector:
Probing sentence embeddings for linguistic properties"

These two papers shed light on some methods researchers have already used to
investigate the semantic (and syntactic) properties captured by current
sentence embeddings. The first paper uses a simple cosine similarity comparison
method to determine how well sentence embeddings capture the semantics of
sentences and how surface-level properties like word order affect the
representation of variable-length sentences as fixed-length vectors. The second
paper discusses the approach of using a set of probing tasks and compared the
results of popular embeddings with baselines such as human judgment.

#### Milestones reached
In my inital project proposal, my first milestone (2/1/19) was deciding on the
four word embeddings to investigate for the remainder of the semester. I am not
there yet, but my next meeting with Professor Wehbe will primarily be on this
topic. We need to decide whether I should look into word embeddings or
sentence embeddings, and determine which embeddings are most easily obtained.
We are constrained by the ability to obtain pretrained embeddings, and since we
are comparing different embeddings, they also should be trained on the same
training set. I have found a few different sources of pretrained vectors as
potential options, such as FastText and GloVe.
