---
layout: post
title: Report 7
categories: [report]
comments: false
---

#### Progress since Report 6
In the past three weeks I have been trying out various Python libraries to run
simple evaluation tasks on my pretrained word embeddings and the residuals
obtained from my previous CCA experiments. I settled on
[GluonNLP](https://gluon-nlp.mxnet.io/), which provides a solid and easy-to-use
framework for intrinsic evaluation of pretrained word embeddings.

I chose to start with intrinsic evaluation methods, similar to the semantic
similarity test described in my previous report, because these tasks are much
more computationally lightweight compared to training a downstream NLP task.

This time, I used the starter code [here](https://gluon-nlp.mxnet.io/examples/word_embedding_evaluation/word_embedding_evaluation.html),
which evaluates the word embeddings on their performance in predicting D in the
analogy "A is to B as C is to D," given A, B, and C.

Consistent with my previous results, GloVe performs worse than Skip-Gram. Also,
as expected, the residuals performed worse than the original embeddings.
However, it is interesting to note that the residual obtained by `sg - pred_sg`
performed significantly worse than just `sg`, where `sg` is the Skip-Gram
embedding and `pred_sg` is the vector predicted by GloVe after training CCA:
Skip-Gram had an accuracy of 0.776, while the residual had an accuracy of 0.66.
On the other hand, the residual obtained by `sg - pred_glove` performed close to
the original, with an accuracy of 0.75. I currently have no explanation or
intuition for this, but hopefully further testing will shed some more light on
this.

#### Checking in with my milestones
Milestone 6: "Next, I will interpret the results from the initial CCA/residuals
experiment. I should also run more experiments to determine which words lead to
the largest differences in word embeddings; the hope is that this reveals more
information about the information that these word embeddings capture through the
training process."

I slightly deviated from this milestone. Rather than continuing the analysis of
the residuals as simply vectors in a vector space, I began using other NLP tasks
to evaluate and compare the embeddings.

#### Next steps
My final steps for this project this semester are to experiment with another
intrinsic task, use the pretrained word embeddings available in GluonNLP, and
obtain residuals from linear regression (rather than CCA).

Once I finish up these tests, I hope to have enough data to present new and
exciting results at Meeting of the Minds.
