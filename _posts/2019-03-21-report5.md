---
layout: post
title: Report 5
categories: [report]
comments: false
---

#### Progress since Report 4
In [my previous report](/research/articles/2019-02/report4.html), I mentioned
extremely low prediction accuracy in my attempt to use CCA to relate word
embeddings from two different models. This was due to not using the correct
number of components in the CCA model.

Since finding the bug, I have begun initial analysis on the CCA results of 3
pre-trained word embeddings obtained using the following models (in 500
dimensions):
1. CBOW
2. Skip-Gram
3. GloVe

While CBOW and Skip-Gram are Word2vec models which try to place words that
appear in similar contexts in the dataset closer in the vector space, GloVe
uses co-occurrence statistics to determine semantic similarity. The similarity
between CBOW and Skip-Gram is reflected in the prediction accuracy plots
shown below.

![Skip-Gram vs. GloVe CCA plot](/research/img/500d_500cc_sg_glove.png)
The plot above shows the results of CCA predictions on Skip-Gram and GloVe,
where the green dots represent the accuracy of predicting Skip-Gram from GloVe
vectors, and the orange dots represent the accuracy of predicting GloVe from
Skip-Gram vectors.
![Skip-Gram vs. CBOW CCA plot](/research/img/sg_cbow_500.png)
The plot above shows the results of CCA predictions on Skip-Gram and CBOW.
![GloVe vs. CBOW CCA plot](/research/img/glove_cbow_500d_250cc_100reg.png)
The plot above shows the results of CCA predictions on GloVe and CBOW.

While the SkipGram-to-CBOW and CBOW-to-SkipGram prediction accuracies are
extremely similar (overlapping), the accuracy of prediction of Skip-Gram/CBOW
vectors given GloVe vectors is consistently worse than that of GloVe vectors
given Skip-Gram/CBOW vectors. This implies that Skip-Gram and CBOW vectors
contain more information then GloVe vectors, given that it's easier to go from
the former to the latter rather than the other way around.

This is consistent with the following papers I read prior to starting my own
analysis:
1. "Evaluation of Word Vector Representations by Subspace Alignment" by
Tsvetkov et al.
2. "Evaluation methods for unsupervised word embeddings" by Schnabel et al.
3. "Exploring Semantic Properties of Sentence Embeddings" by Zhu et al.

In each of these papers, the evaluation techniques used showed that Word2vec
models perform better than the GloVe model.

The next step is to think of additional computations that will shed more light
on what my results truly mean and perhaps confirm what I've inferred here. There
could be a confounding variable in my CCA "experiment" that resulted in
misleading plots. 
