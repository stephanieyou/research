---
layout: post
title: Report 6
categories: [report]
comments: false
---

#### Progress since Report 5
In the last two weeks, I have been using residuals obtained from my previous
CCA results and performing a simple semantic similarity test.

To calculate the semantic similarity of two word embeddings x and y, I simply
found the normalized inner product of x and y. To visualize the results, I
generated a heat map such that more similar pairs of words are darker on the
map than less similar pairs of words. The code for this was adapted from
[an example Colaboratory notebook](https://colab.research.google.com/github/
tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_
universal_encoder.ipynb#scrollTo=RUymE2l9GZfO).

I ran this test on the following sets of embeddings/vectors:
* GloVe word embeddings
* Skip-Gram word embeddings
* The residual vectors obtained by subtracting the predicted GloVe embedding
from the original GloVe embedding
* The residual vectors obtained by subtracting the predicted Skip-Gram embedding
from the original Skip-Gram embedding

Here are the results:

![GloVe heat map](/research/img/glove_nonzscored.png)
![GloVe residual heat map](/research/img/glove_sg_residual.png)
![Skip-Gram heat map](/research/img/sg_nonzscored.png)
![Skip-Gram residual heat map](/research/img/sg_glove_residual.png)

The interesting observation is that the residual vectors don't seem to lose much
(if any) information about semantic similarity, since the heat maps are all
pretty similar and capture the actual semantic similarity of the words.

#### Checking in with my milestones
Milestone 5: "After replicating another paperâ€™s results, I should be able to
confidently use CCA (canonical correlation analysis) and/or regression residuals
to find similarities and differences between word embeddings."

I have achieved this milestone at this point, having used CCA and running the
above semantic similarity test on the regression residuals.

#### Next steps
The results with the simple semantic similarity test are promising (with regards
to the residuals) - the next step is to do an actual NLP task (i.e. a downstream
task) with the original word embeddings and with the residuals, and see if there
is a significant change in performance.
